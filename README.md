# ðŸ”„ Data Streaming with Kafka: MySQL â†’ Debezium â†’ Kafka â†’ Python â†’ CSV

Este proyecto implementa un pipeline de datos en tiempo real utilizando tecnologÃ­as modernas de captura de cambios (CDC) para extraer y almacenar datos actualizados desde una base de datos MySQL hacia archivos CSV (enviados y transformados en Python) para posterior anÃ¡lisis o integraciÃ³n.

---

## ðŸ“Œ Objetivo

DiseÃ±ar un flujo de procesamiento en tiempo real que:
- Detecte cambios en una tabla de MySQL (inserts, updates, deletes)
- Capture esos eventos utilizando el conector de Debezium
- Transmita los eventos a travÃ©s del topico definido en Apache Kafka
- Consuma los datos con Python
- Almacene los resultados en un archivo `CSV` con formato estructurado

---

## ðŸ§± Arquitectura general

```plaintext
   +--------+       +-------------+       +--------+       +--------+       +-----------+
   | MySQL  | <---> | Debezium    | <---> | Kafka  | <---> | Python | --->  | CSV final |
   +--------+       +-------------+       +--------+       +--------+       +-----------+
     Fuente         CDC Connector        MensajerÃ­a       Consumidor        Almacenamiento

   +--------+       +-------------+       +--------+       +--------+       +-----------+
   | MySQL  | <---> | Debezium    | <---> | Kafka  | <---> | Python | --->  | CSV final |
   +--------+       +-------------+       +--------+       +--------+       +-----------+
     Fuente         CDC Connector        MensajerÃ­a       Consumidor        Almacenamiento
```

![Arquitectura del pipeline](img/Arquitectura_de_datos.png)
